# -*- coding: utf-8 -*-
"""CamemBERT_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13yw_OyuNbGbqdBkMEvcd0EiksrTkDkXG

# Text Embeddings

### Explanation

This model uses **CamemBERT Text Embeddings** to analyse the Language Level of French text. In addition to the usual elements of CamemBERT, our code implements several additional features:

**Data Augmentation:** We created additional data by replacing synonyms, deleting, inserting or swapping random sentences.
"""

# Do you want to augment your data?
data_augmentation = False
# If yes, how many sentences do you want to generate
# per sentence in the training dataset?
multiplication = 1

"""**Scheduled learning rate:** Every epoch, the learning rate is multiplied by a factor such that it gets lower for higher epochs."""

# Do you want to implement scheduling?
scheduling = True
# By what factor should the learning rate be
# multiplied per epoch?
scheduling_factor = 0.5

"""**Focal Loss:** Instead of simply using Cross Entropy Loss, we implemented Focal Loss, which adds to Cross Entropy Loss by specifically focussing on misclassified sentences:"""

# This factor determines by how much misclassified samples
# are going to be prioritizes (0: no prioritization)
gamma_FL = 1.4

"""**Dropout:** The configuration of the CamemBERT is such that a fraction of the outputs of the neurons is ignored, ensuring generalization."""

# The fraction of neurons that is ignored:
dropout_fraction = 0.05

"""**L2 regularization:** The parameter weight_decay ensures that the optimizers does not give too much importance to certain weights."""

# Give a number between 0.1 (high differences)
# and 0.001 (low differences):
L2_decay = 0.01

"""**Early stopping:** Instead of training the model on multiple numbers of iterations and learning rates (aka "Hyperparameter Tuning"), the model stops when the loss has not decreased for more than *patience* number of epochs. This allows us to run the code multiple times with different learning rates, without having to optimize the number of iterations. This avoids high computation time."""

# After how many epochs without improvement
# does the code stop?
patience = 2

"""##Imports"""

import pandas as pd
import numpy as np
import torch
from transformers import CamembertTokenizer, CamembertForSequenceClassification, AdamW
from transformers import CamembertModel, CamembertConfig
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

"""## Functions

### Label encoding
"""

# Functions
def score2number(df):
    results = []
    scores = df['difficulty']
    for score in scores:
        if score == "A1":
            results.append(0)
        if score == "A2":
            results.append(1)
        if score == "B1":
            results.append(2)
        if score == "B2":
            results.append(3)
        if score == "C1":
            results.append(4)
        if score == "C2":
            results.append(5)
    return results

def number2score(df):
    results = []
    numbers = df['numeric']
    for number in numbers:
        if number < 0.5:
            results.append("A1")
        if round(number) == 1:
            results.append("A2")
        if round(number) == 2:
            results.append("B1")
        if round(number) == 3:
            results.append("B2")
        if round(number) == 4:
            results.append("C1")
        if number >= 4.5:
            results.append("C2")
    return results

"""### Data Augmentation"""

import random
from nltk.corpus import wordnet
import nltk
nltk.download('wordnet')

def synonym_replacement(text, n):
    words = text.split()
    new_words = words.copy()
    random_word_list = list(set([word for word in words if word not in set(wordnet.words())]))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(list(synonyms))
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:  # Only replace up to n words
            break

    return ' '.join(new_words)

def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lem in syn.lemmas():
            synonym = lem.name().replace('_', ' ').replace('-', ' ').lower()
            synonyms.add(synonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def random_deletion(text, p):
    words = text.split()
    if len(words) == 1:
        return words
    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        return [words[random.randint(0, len(words) - 1)]]
    return ' '.join(new_words)

def random_swap(text, n):
    words = text.split()
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return ' '.join(new_words)

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words) - 1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_2 == random_idx_1:
        random_idx_2 = random.randint(0, len(new_words) - 1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
    return new_words

def random_insertion(text, n):
    words = text.split()
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return ' '.join(new_words)

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words) - 1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = synonyms[0]
    random_idx = random.randint(0, len(new_words) - 1)
    new_words.insert(random_idx, random_synonym)

def generate_augmented_sentences(text, num_augmented_sentences=4):
    augmented_sentences = []
    for _ in range(num_augmented_sentences):
        augmentation_choice = random.choice([
            lambda x: synonym_replacement(x, n=1),
            lambda x: random_deletion(x, p=0.1),
            lambda x: random_swap(x, n=1),
            lambda x: random_insertion(x, n=1)
        ])
        augmented_sentences.append(augmentation_choice(text))
    return augmented_sentences

"""### Evaluate a CamemBERT model:"""

import torch
from transformers import CamembertModel, CamembertConfig, CamembertTokenizer
def evaluate_camembert_model(config_file, model_file, input_text):
    # Load model configuration
    config = CamembertConfig.from_pretrained(config_file)

    # Initialize model with the loaded configuration
    model = CamembertForSequenceClassification(config)

    # Load the state dictionary
    state_dict = torch.load(model_file, map_location=torch.device('cpu'))

    # Load the state dictionary into the model
    model.load_state_dict(state_dict)

    # Set the model to evaluation mode
    model.eval()

    # Tokenize input text
    tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
    inputs = tokenizer(input_text, return_tensors="pt")

    # Forward pass
    with torch.no_grad():
        outputs = model(**inputs)

    # Get the output tensor (logits)
    logits = outputs.logits

    # Get the predicted class index
    predicted_class_idx = torch.argmax(logits, dim=1).item()

    return predicted_class_idx

"""## Classes

### Language Level Dataset

Transforms sentences and language levels:
"""

# Define a custom dataset class
class LanguageLevelDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': label  # Return labels as a simple tensor
        }

"""### Focal loss"""

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, input, target):
        ce_loss = F.cross_entropy(input, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        else:
            return focal_loss

"""## Execution

Parameters of the CamemBERT model:
"""

# How many sentences does the
# model evaluation at once?
BatchSize = 20
# Layers of the neural network
NumLayers = 17
# Neurons per layer
PerLayer = 768
# Define the hyperparameters
learning_rates = [5e-5]
num_iterations = [400]

"""The final code:"""

# Data downloads
df_training = pd.read_csv('https://raw.githubusercontent.com/juliw9/FrenchLanguageClassifier/main/training_data.csv')
df_training["numeric"] = score2number(df_training)

df_test = pd.read_csv('https://raw.githubusercontent.com/juliw9/FrenchLanguageClassifier/main/unlabelled_test_data.csv')

train_df, val_df = train_test_split(df_training, test_size=0.05, random_state=42)  # 80-20 split

# Initialize the Focal Loss function
focal_loss_fn = FocalLoss(alpha=1, gamma=gamma_FL)

label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(train_df["difficulty"])
y_val_encoded = label_encoder.transform(val_df["difficulty"])

best_performance = float('-inf')
best_hyperparameters = {}

# Retrain the model on the whole dataset with the best hyperparameters
model_name = "camembert-base"
tokenizer = CamembertTokenizer.from_pretrained(model_name)
config = CamembertForSequenceClassification.config_class.from_pretrained(model_name)
config.num_labels = len(label_encoder.classes_)  # Number of language levels

# Manually set the number of layers and neurons per layer
config.num_hidden_layers = NumLayers  # Set the number of layers
config.hidden_size = PerLayer  # Set the number of neurons per layer

# Avoiding overfitting:
config.dropout = dropout_fraction

# Initialize the model with customized configuration
model = CamembertForSequenceClassification.from_pretrained(model_name, config=config)
optimizer = AdamW(model.parameters(), lr=learning_rates[0], no_deprecation_warning=True, weight_decay = L2_decay)

# Define the scheduler:
if scheduling:
  scheduler = StepLR(optimizer,step_size=1, gamma = scheduling_factor)

original_sentences = list(train_df["sentence"])
val_sentences = list(val_df["sentence"])
original_labels = list(train_df["numeric"])
val_labels = list(val_df["numeric"])

augmented_sentences = []
augmented_labels = []

print("Data augmentation:")
if data_augmentation:
  augmentation_counter = 0
  tencounter = 0
  multiplication = 1
  for sentence, label in zip(original_sentences, original_labels):
      tencounter += 1
      augmentation_counter += 1
      if tencounter == 100:
        tencounter = 0
        print(f"{augmentation_counter*(multiplication)} sentences generated.")
      new_sentences = generate_augmented_sentences(sentence, num_augmented_sentences=multiplication)
      augmented_sentences.extend(new_sentences)
      augmented_labels.extend([label] * len(new_sentences))
  print("Completed")
else:
  print("Not implemented.")

# Combine original and augmented data
all_sentences = original_sentences + augmented_sentences
all_labels = original_labels + augmented_labels

print(f"Original sentences: {len(original_sentences)}")
print(f"Augmented sentences: {len(augmented_sentences)}")
print(f"Total sentences: {len(all_sentences)}")

max_length = 64
# Enable augmentation for training dataset
train_dataset = LanguageLevelDataset(all_sentences, all_labels, tokenizer, max_length)
train_dataloader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True)

val_unlabelled = val_df["sentence"]
val_dataset = LanguageLevelDataset(val_unlabelled.to_list(), [0]*len(val_unlabelled), tokenizer, max_length)
val_dataloader = DataLoader(val_dataset, batch_size=BatchSize, shuffle=False)

# Preparation of evaluation:
X_unlabelled = df_test["sentence"]
X_unlabelled_encoded = LanguageLevelDataset(X_unlabelled.tolist(), [0]*len(X_unlabelled), tokenizer, max_length)  # Dummy labels, not used during inference
test_dataloader = DataLoader(X_unlabelled_encoded, batch_size=BatchSize, shuffle=False)
predicted_levels = []

model.train()
print("Training:")
best_loss = float('inf')
counter = 0
for epoch in range(num_iterations[0]):
    print(f"Epoch {epoch+1}/{num_iterations[0]}")

    total_loss = 0
    batchcounter = 1
    for batch in train_dataloader:
      if (batchcounter % 50) == 0:
        print(f"\t{batchcounter} of {len(train_dataloader)}")
      batchcounter += 1
      input_ids = batch['input_ids']
      attention_mask = batch['attention_mask']
      labels = batch['labels']

      optimizer.zero_grad()
      outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
      loss = focal_loss_fn(outputs.logits, labels)
      total_loss += loss.item()

      loss.backward()
      optimizer.step()

    if scheduling:
      scheduler.step()
    avg_loss = total_loss / len(train_dataloader)
    print(f"Average loss {avg_loss}:")
    if avg_loss < best_loss:
      print("Improved")
      best_loss = avg_loss
      counter = 0

      model.eval()
      predicted_levels = []
      print("Accuracy evaluation:")
      for batch in val_dataloader:
          input_ids = batch['input_ids']
          attention_mask = batch['attention_mask']

          with torch.no_grad():
              outputs = model(input_ids=input_ids, attention_mask=attention_mask)

          predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()
          predicted_levels.extend(predicted_labels)

      y_pred = predicted_levels
      y_test = y_val_encoded
      # Evaluate the accuracy, F1 score, recall, and precision
      accuracy = accuracy_score(y_test, y_pred)
      f1 = f1_score(y_test, y_pred, average='weighted')
      recall = recall_score(y_test, y_pred, average='weighted')
      precision = precision_score(y_test, y_pred, average='weighted')

      print(f'Test Set Accuracy: {accuracy:.2f}')
      print(f'Test Set F1 Score: {f1:.2f}')
      print(f'Test Set Recall: {recall:.2f}')
      print(f'Test Set Precision: {precision:.2f}')

      if avg_loss < 2:
        # Evaluation:
        model.eval()
        predicted_levels = []
        for batch in test_dataloader:
          input_ids = batch['input_ids']
          attention_mask = batch['attention_mask']

          with torch.no_grad():
              outputs = model(input_ids=input_ids, attention_mask=attention_mask)

          predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()
          predicted_levels.extend(predicted_labels)
        #modelname = "12Mai_Iteration" + str(epoch)
        model.save_pretrained("model_name", save_weights_only=True)
        torch.save(model.state_dict(), "model_name/pytorch_model.bin")

        # Add predicted language levels to the unlabeled dataframe
        df_test["numeric"] = predicted_levels

        # Export
        df_test["difficulty"] = number2score(df_test)
        df_test.loc[:,["id","difficulty"]].to_csv("submission.csv",index=False)

    else:
      counter += 1
      if counter > patience:
        print(f"Model has not improved for {counter} steps, after {epoch + 1} total steps.")
        break
      else:
        print(f"Suboptimal - counter set to {counter}.")

"""## Evaluation

Once the model saved in the two files config.json and model.safetensors, it can be evaluated as follows:
"""

config_file = "model_name/config.json"
model_file = "model_name/pytorch_model.bin"
input_text = "Bonjour, comment Ã§a va?"

predicted_class_idx = evaluate_camembert_model(config_file, model_file, input_text)

# To convert the predicted class index to the actual language level, use the label encoder
predicted_level = label_encoder.inverse_transform([predicted_class_idx])
print(predicted_level[0])

"""To evaluate our best model so far, we have to transform its model.safetensors file into a pytorch_model.bin file:"""

# Load the configuration
config = CamembertConfig.from_pretrained("sample_data/config.json")

# Load the model
model = CamembertModel.from_pretrained("sample_data/model.safetensors", config=config)

# Save the model's state_dict() as pytorch_model.bin
torch.save(model.state_dict(), "pytorch_model.bin")

"""Evaluate the training dataset:"""

config_file = "sample_data/config.json"
model_file = "sample_data/pytorch_model.bin"

test_sentences = list(training_data["sentence"])
y_test = list(training_data["difficulty"])

y_pred = []
for test_sentence in test_sentences:
  predicted_class_idx = evaluate_camembert_model(config_file, model_file, test_sentence)
  predicted_level = label_encoder.inverse_transform([predicted_class_idx])
  y_pred.append(predicted_level)

"""Finally, evaluate the model:"""

# Evaluate the accuracy, F1 score, recall, and precision
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted')

print(f'Test Set Accuracy: {accuracy:.2f}')
print(f'Test Set F1 Score: {f1:.2f}')
print(f'Test Set Recall: {recall:.2f}')
print(f'Test Set Precision: {precision:.2f}')