The goal of the project is to predict the level of difficulty of a sentence in French language. Difficulty levels include A1, A2, B1, B2, C1 and C2.
There is some training data given in order to train the model which is done with 80% of this data. The leftover randomly chosen 20% is used for testing. Additioanlly, there is also some unlabelled test data that the model was tested on to achieve some general metrics associated with the testing. These metrics include precision, recall, F1 score and accuracy.
Before cosnidering deeper analysis of this project, pre-developed classifiers were implemented into predcition, including KNN, Decision Tree, Random Forest and Logistic Regression. Metric results of these classifiers are included in the table. KNN performs the worst among the classifiers in this scenario. KNN classifier relies on the local structure of the data. It may not capture complex relationships between sentences and difficulties, leading to poorer performance compared to others. Decision Tree performs similarly to KNN but slightly better. Decision Trees can overfit easily while dealing with high-dimensional data such as text. This likely caused poorer generalization and performance on unlabelled data. Random Forest outperforms KNN and Decision Tree. Random Forests reduces the overfitting impact by combining multiple decision trees through ensemble learning. Combination of decision trees hekos in understanding the complex relationships between sentences and difficulty level and generalize the model. Logistic Regression performs the best among the pre-developed classifiers. Logistic Regression is well-suited for multiclass classification (six difficulty levels) when used with multinomial logistic regression. One-vs-Rest (OvR) technique within logistic regression did not perform better than multinomial due to worse smoothness of decision boundaries and imbalanced class distributions for individual binary classifiers. The CEFR levels have a natural ordinal relationship, indicating a progression in language proficiency from A1 to C2. Multinomial logistic regression considers the ordinal relationship among the classes and can model the probability of each class directly, capturing the inherent order. The smoothness of the boundaries is beneficial when the classes are not well-separated, as is often the case in ordinal classification with smaller number of classes (only six in this case). A single softmax function created linear decision boundary which turned out to be effective in capturing the patterns in the data, resulting in better performance compared to any other classifier. Although multinomial logistic regression gave quite satisfying results for such a short amount of code, it was only the motivation and the beginning of journey to achieve higher accuarcy.
The model that team Hublot created uses Camembert model, specifically designed for classifying difficulty levels of French language. This model has already been pre-trained to some extent on other data. The final version of the actual model contains features that improve the simple model. This editing required significant number of libraries. Two utility functions were defined for converting between difficulty scores and numeric representations and vice versa. This was done to make the comparison easier. This data augmentation code generates augmented versions of a given text by applying various operations randomly. These operations include synonym replacement, random deletion, random swap, and random insertion of words. The generate_augmented_sentences function utilizes these methods to create multiple augmented sentences, enhancing the diversity of the training data and potentially improving the generalization and performance of natural language processing models.
