The goal of the project is to predict the level of difficulty of a sentence in French language. Difficulty levels include A1, A2, B1, B2, C1 and C2. In order to navigate the github page, there are three sections, including code, data, and streamlit app. The code sections contains the final version of the code in its significant parts. Every part represents a separate unit of the code. All units combined contributed to the accuracy of 0.618 which placed team Hublot at the fifth place. The data section contains two files of training data (what the model was trained on) and the unlabelled test data (what model was tested on to achieve the end accuracy). The section about the streamlit app contains the 23 parts of the actual code, the configuration file, the streamlit code necessary for the application operation, and an image of the app homepage.

There is some training data given in order to train the model which is done with 80% of this data. The leftover randomly chosen 20% is used for testing. Additioanlly, there is also some unlabelled test data that the model was tested on to achieve some general metrics associated with the testing. These metrics include precision, recall, F1 score and accuracy.

Before cosnidering deeper analysis of this project, pre-developed classifiers were implemented into predcition, including KNN, Decision Tree, Random Forest and Logistic Regression. Metric results of these classifiers are included in the table. KNN performs the worst among the classifiers in this scenario. KNN classifier relies on the local structure of the data. It may not capture complex relationships between sentences and difficulties, leading to poorer performance compared to others. Decision Tree performs similarly to KNN but slightly better. Decision Trees can overfit easily while dealing with high-dimensional data such as text. This likely caused poorer generalization and performance on unlabelled data. Random Forest outperforms KNN and Decision Tree. Random Forests reduces the overfitting impact by combining multiple decision trees through ensemble learning. Combination of decision trees hekos in understanding the complex relationships between sentences and difficulty level and generalize the model. Logistic Regression performs the best among the pre-developed classifiers. Logistic Regression is well-suited for multiclass classification (six difficulty levels) when used with multinomial logistic regression. One-vs-Rest (OvR) technique within logistic regression did not perform better than multinomial due to worse smoothness of decision boundaries and imbalanced class distributions for individual binary classifiers. The CEFR levels have a natural ordinal relationship, indicating a progression in language proficiency from A1 to C2. Multinomial logistic regression considers the ordinal relationship among the classes and can model the probability of each class directly, capturing the inherent order. The smoothness of the boundaries is beneficial when the classes are not well-separated, as is often the case in ordinal classification with smaller number of classes (only six in this case). A single softmax function created linear decision boundary which turned out to be effective in capturing the patterns in the data, resulting in better performance compared to any other classifier. Although multinomial logistic regression gave quite satisfying results for such a short amount of code, it was only the motivation and the beginning of journey to achieve higher accuarcy.

The model that team Hublot created uses Camembert model, specifically designed for classifying difficulty levels of French language. This model has already been pre-trained to some extent on other data. The final version of the actual model contains features that improve the simple model. This editing required significant number of libraries. Two utility functions were defined for converting between difficulty scores and numeric representations and vice versa. This was done to make the comparison easier. The data augmentation process in the code generates augmented versions of a given text by applying various operations that include synonym replacement, random deletion, random swap, and random insertion of words. The generate_augmented_sentences function combines these methods to create multiple augmented sentences, enhancing the diversity of the training data and potentially improving the generalization and performance of the natural language model. New function is introduced to evaluate the Camembert model. This function takes a configuration file and a model file of a Camembert model. It uses these files to load a pre-trained model. Then, it evaluates the model's prediction on a given input text. This involves processing the text using the model's tokenizer, passing it through the model, and obtaining the predicted class index based on the model's output. A custom dataset class called 'LanguageLevelDataset' is defined and used to prepare data for training a model. The class takes input texts, corresponding labels, a tokenizer, and a maximum length parameter. It determines the length of the dataset and retrieves individual data samples. Each data sample consists of the input text encoded using the provided tokenizer, along with its corresponding label, both formatted as tensors. Custom loss function was used to adjust the standard cross-entropy loss by giving more weight to hard-to-classify examples. It calculates focal loss based on the input predictions and target labels, allowing for customization of alpha and gamma parameters to control the loss function's behavior. The loss can be computed as a mean, sum, or without reduction, depending on the specified reduction parameter. Once these functions have been created, the actual running code begins. It begins by setting up hyperparameters, loading training and test data, and initializing the model and optimizer. Hyperparameter modification played a key role in increasing the accuracy of the model. Larger number of epochs seemed to improve the accuracy. However, it does so up to certain treshold where it starts decreasing later on. Data augmentation increases the diversity of the training data. The augmented and original data were combined. Each training epoch consisting of batches of sentences passed through the model. Batches were also optimized based on the number of epochs for better performance. The number of iterations was set to be large but on purpose since not all of them were used. The reason behind this is that the focal loss function was used as the optimization criterion, aiming to minimize the loss between predicted and true labels. During training, if the loss improved, the model was evaluated on the test data. If the performance surpassed a certain threshold, the model parameters were saved. This process continued until a predefined number of iterations or until the loss stops improving. Finally, the predicted language levels for the test data were exported to a CSV file for submission. This model had an accuracy of 0.618, which is far beyond the pre-developed models initially used.
